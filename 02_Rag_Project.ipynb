{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZLN5Lyo5TGoT9rZ6FCoY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhadAli451/PIAIC-Quarter-2-Projects/blob/main/02_Rag_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This is a first way to build embedding model with pinecone and through the langchain and add some like pinecone pineconeVectorStore\n",
        "\n",
        "### 1 below write code to create embedding model\n",
        "### 2 build a pineconevectorstore"
      ],
      "metadata": {
        "id": "nfLE9hJwQcOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU  langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1w13_Zzk6Gi",
        "outputId": "c3b755d0-2331-430d-9762-eebc90ebba9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "qnD4fUUflXte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"rag-project\"  # change if desired\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "6uz9AgrulcBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vector = embeddings.embed_query(\"how does work ai\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyLy3qeals0l",
        "outputId": "6506049c-290a-4479-e93b-4150520f6c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.005915421061217785,\n",
              " -0.06183977797627449,\n",
              " -0.027067674323916435,\n",
              " -0.005238934885710478,\n",
              " -0.012123123742640018]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import pineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "jual4nKMlwrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_11 = Document(\n",
        "    page_content=\"\"\"AI systems work by combining large sets of data with intelligent,\n",
        "     iterative processing algorithms to learn from patterns and features in the data\n",
        "     that they analyze. Each time an AI system runs a round of data processing, it\n",
        "      tests and measures its own performance and develops additional expertise. :(\"\"\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "    document_11,\n",
        "]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R1XkUFnnHOc",
        "outputId": "b6f741f8-45b1-478f-b5e9-bd38096a591d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['80f97987-509a-422b-bdb0-d2928ca6cb91',\n",
              " '56231b12-4dc7-40c8-ae27-5f059ca6a8e3',\n",
              " 'a36d7d76-f08c-434f-8858-6661eeda4817',\n",
              " 'f8f3433a-5384-4a45-895e-fc87410cc3de',\n",
              " '29b69054-9602-41ee-be10-7c84558f148a',\n",
              " '2bfbcb81-8180-496b-bbc6-d2144978de38',\n",
              " '3afcd0a5-dade-4609-b3da-7405d99ef09e',\n",
              " '15b7f232-dc14-4242-8ae4-94ae955b3fb9',\n",
              " '2ace1a1e-53c4-478f-b8fc-602cdd0587be',\n",
              " '4f9bfa66-4333-42a4-99da-5e4ac2ff416c',\n",
              " '17b44cc3-d407-4997-8a3d-ef5bc212c39f']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iSU2Kw1qF_i",
        "outputId": "4878d6a6-4a7f-4ea7-84ee-084b9bb8dee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Reterive\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    \"how does ai work\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
        "\n",
        "# Data Reterive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWIGuOxAnWII",
        "outputId": "a11a2f22-b4a4-434b-ffb3-06a3d71936cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* AI systems work by combining large sets of data with intelligent,\n",
            "     iterative processing algorithms to learn from patterns and features in the data \n",
            "     that they analyze. Each time an AI system runs a round of data processing, it\n",
            "      tests and measures its own performance and develops additional expertise. :( [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"how does work ai\",k=2,filter={\"source\": \"tweet\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMtwe_rQnabW",
        "outputId": "e7ffdc0b-de54-4c89-86aa-bbfd6663d959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.655197] AI systems work by combining large sets of data with intelligent,\n",
            "     iterative processing algorithms to learn from patterns and features in the data \n",
            "     that they analyze. Each time an AI system runs a round of data processing, it\n",
            "      tests and measures its own performance and develops additional expertise. :( [{'source': 'tweet'}]\n",
            "* [SIM=0.553384] Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "cU4LloUnoVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_for_user(query: str):\n",
        "\n",
        "\n",
        "  #Vector Serech\n",
        "  vector_results = vector_store.similarity_search(query, k=2)\n",
        "  print(len(vector_results))\n",
        "\n",
        "  # TODO: pass to Model Vector Result + User Query\n",
        "  final_answer = llm.invoke(f\"This is a user query: {query}, Here are some references to answer{vector_results} \")\n",
        "\n",
        "\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "_kIjuxl-oY2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_for_user(\"How does work ai\")\n",
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "9JGzD7Vdo4tH",
        "outputId": "2c492b28-7c36-4828-c6a2-b86b23ae15d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the provided text, AI systems work by:\\n\\n1. **Combining large datasets with intelligent algorithms:**  AI uses massive amounts of data as input.  This data is processed using sophisticated algorithms designed to identify patterns and features.\\n\\n2. **Iterative processing and learning:** The algorithms don't just process the data once. They repeatedly analyze the data, testing their performance and refining their understanding of the patterns within the data with each iteration.  This iterative process allows the AI to learn and improve its accuracy over time.\\n\\nThe second document is irrelevant to the question of *how* AI works.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Secound Work For Embedding"
      ],
      "metadata": {
        "id": "SUXkq2zq6Epx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pakistan zinda bad, we love our country.\n",
        "# 0         1     2    3  4.   5.   6\n"
      ],
      "metadata": {
        "id": "3NdBD3qy5lMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "VSXabW5P54Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "7oAb-2up6AC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(genai.list_models())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IDxDQ26_6Lfr",
        "outputId": "2f08b9c1-9bb2-4528-94aa-c017b2d7ae64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
              "                    'February 15th, 2025. Move to a newer Gemini version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
              "                    'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
              "                    'version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
              "                    'million tokens.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Pro 002',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
              "                    'across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
              "                    'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
              "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Flash 002',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
              "                    'released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
              "       description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Experimental',\n",
              "       description='Gemini 2.0 Flash Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-exp-1206',\n",
              "       base_model_id='',\n",
              "       version='exp_1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1121',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1114',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/learnlm-1.5-pro-experimental',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='LearnLM 1.5 Pro Experimental',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
              "                    'mid-size multimodal model that supports up to 2 million tokens.'),\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Dict\n",
        "\n",
        "result : Dict = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=\"What is the meaning of life?\",\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of single string\",\n",
        ")\n",
        "\n",
        "# # 1 input > 1 vector output\n",
        "# print(str(result[\"embedding\"])[:50], \"... TRIMMED]\")\n",
        "\n",
        "result['embedding']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bBDj5-nY6Rka",
        "outputId": "bdbb8269-752a-4273-be88-0961fe22dd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.02854543,\n",
              " 0.044588115,\n",
              " -0.034197364,\n",
              " -0.0042663575,\n",
              " -0.04079577,\n",
              " 0.012999958,\n",
              " 0.018053582,\n",
              " 0.06015144,\n",
              " -0.0028713925,\n",
              " 0.009951648,\n",
              " 0.024832657,\n",
              " -0.01683923,\n",
              " 0.09940116,\n",
              " -0.031990346,\n",
              " 0.018328529,\n",
              " -0.109134205,\n",
              " 0.001190296,\n",
              " 0.0014311911,\n",
              " -0.083155245,\n",
              " -0.010203233,\n",
              " 0.019211812,\n",
              " 0.0010217889,\n",
              " 0.053874534,\n",
              " -0.0150861535,\n",
              " -0.003189089,\n",
              " 0.019626662,\n",
              " -0.0074312133,\n",
              " -0.036586244,\n",
              " -0.008509182,\n",
              " -0.017352631,\n",
              " 0.058202818,\n",
              " 0.05446324,\n",
              " 0.01571296,\n",
              " -0.021822602,\n",
              " 0.048009068,\n",
              " 0.022641798,\n",
              " -0.0069730366,\n",
              " 0.054272633,\n",
              " 0.025922865,\n",
              " -0.027334303,\n",
              " -0.07256842,\n",
              " 0.028509492,\n",
              " -0.03564165,\n",
              " 0.060492564,\n",
              " -0.022731686,\n",
              " -0.030770157,\n",
              " -0.006176277,\n",
              " -0.021891864,\n",
              " -0.019659325,\n",
              " 0.0643669,\n",
              " 0.03154234,\n",
              " 0.017379418,\n",
              " -0.03679774,\n",
              " 0.016511764,\n",
              " -0.02536976,\n",
              " -0.022270117,\n",
              " -0.012396498,\n",
              " -0.032805424,\n",
              " 0.054154944,\n",
              " -0.04823156,\n",
              " -0.021759441,\n",
              " -0.03370158,\n",
              " -0.025460402,\n",
              " -0.017531719,\n",
              " -0.052902102,\n",
              " 0.04005264,\n",
              " -0.022417234,\n",
              " 0.023286799,\n",
              " -0.081740536,\n",
              " 0.057951722,\n",
              " -0.009130241,\n",
              " 0.040680293,\n",
              " -0.026519705,\n",
              " 0.03940553,\n",
              " -0.009711097,\n",
              " 0.034678303,\n",
              " 0.022007594,\n",
              " 0.013833644,\n",
              " -0.037151057,\n",
              " 0.03425778,\n",
              " -0.037425302,\n",
              " 0.0133839715,\n",
              " 0.062417556,\n",
              " 0.05847619,\n",
              " 0.010972301,\n",
              " 0.0069226264,\n",
              " -0.028345693,\n",
              " -0.025934424,\n",
              " -0.08300387,\n",
              " -0.018701842,\n",
              " 0.051876634,\n",
              " 0.028144632,\n",
              " 0.0050556627,\n",
              " 0.032798667,\n",
              " 0.065246835,\n",
              " -0.030897865,\n",
              " -0.05739043,\n",
              " -0.075857975,\n",
              " 0.013478357,\n",
              " 0.06728078,\n",
              " 0.058566026,\n",
              " 0.004352499,\n",
              " 0.004930005,\n",
              " -0.04687863,\n",
              " 0.047707375,\n",
              " 0.09346361,\n",
              " -0.033218864,\n",
              " -0.031682696,\n",
              " -0.035492342,\n",
              " -0.01704582,\n",
              " 0.0070146834,\n",
              " -0.070768684,\n",
              " 0.033927422,\n",
              " -0.05199852,\n",
              " 0.023873666,\n",
              " -0.07814866,\n",
              " -0.0045958003,\n",
              " 0.008596137,\n",
              " -0.014557764,\n",
              " -0.005185891,\n",
              " 0.019220637,\n",
              " 0.06736503,\n",
              " -0.0036469845,\n",
              " 0.029749395,\n",
              " 0.06519027,\n",
              " -0.0017471175,\n",
              " 0.0032194257,\n",
              " -0.058492143,\n",
              " -0.04441574,\n",
              " -0.02082184,\n",
              " 0.09642446,\n",
              " -0.1019518,\n",
              " -0.014023612,\n",
              " 0.0124329,\n",
              " -0.03430263,\n",
              " 0.017259782,\n",
              " 0.09509829,\n",
              " -0.0046976367,\n",
              " 0.014962477,\n",
              " -0.014655782,\n",
              " 0.013355405,\n",
              " -0.025665203,\n",
              " -0.044942174,\n",
              " 0.025182499,\n",
              " 0.017465375,\n",
              " -0.03649158,\n",
              " 0.041676275,\n",
              " 0.0115632955,\n",
              " -0.059281666,\n",
              " -0.0076066344,\n",
              " -0.020777298,\n",
              " -0.02422031,\n",
              " 0.062161032,\n",
              " 0.004218794,\n",
              " -0.008247845,\n",
              " 0.0025803575,\n",
              " 0.063738205,\n",
              " -0.0257109,\n",
              " 0.095686235,\n",
              " -0.027317889,\n",
              " -0.0018857537,\n",
              " -0.078989305,\n",
              " -0.03540763,\n",
              " 0.0067127054,\n",
              " -0.048317876,\n",
              " -0.030285196,\n",
              " 0.023211101,\n",
              " -0.0491988,\n",
              " 0.010719925,\n",
              " 0.02444096,\n",
              " -0.0052104676,\n",
              " 0.008697184,\n",
              " -0.07112967,\n",
              " -0.033053268,\n",
              " -0.024818258,\n",
              " 0.0080466885,\n",
              " -0.022770239,\n",
              " -0.0047805742,\n",
              " -0.010209843,\n",
              " 0.03673981,\n",
              " 0.09795194,\n",
              " 0.03386203,\n",
              " -0.010665188,\n",
              " -0.07381909,\n",
              " 0.00068866805,\n",
              " -0.025717223,\n",
              " -0.0027137904,\n",
              " 0.04281671,\n",
              " 0.034393556,\n",
              " 0.059981614,\n",
              " -0.056784473,\n",
              " 0.030799013,\n",
              " 0.0035953694,\n",
              " 0.034763683,\n",
              " 0.014165773,\n",
              " -0.04174826,\n",
              " 0.061088957,\n",
              " -0.012045537,\n",
              " -0.051974565,\n",
              " -0.0055955644,\n",
              " 0.007566413,\n",
              " -0.03619617,\n",
              " -0.0193453,\n",
              " -0.038998786,\n",
              " 0.016063262,\n",
              " -0.0075189867,\n",
              " -0.045611285,\n",
              " -0.04747799,\n",
              " 0.047357876,\n",
              " 0.0055609345,\n",
              " -0.029912258,\n",
              " -0.01027596,\n",
              " -0.009503956,\n",
              " -0.022230182,\n",
              " 0.047264725,\n",
              " 0.033944495,\n",
              " 0.0636137,\n",
              " -0.0061410535,\n",
              " 0.10799254,\n",
              " 0.023873555,\n",
              " -0.024063213,\n",
              " -0.01934859,\n",
              " -0.019175187,\n",
              " -0.02375712,\n",
              " 0.010766843,\n",
              " 0.010511031,\n",
              " -0.020831015,\n",
              " -0.0415524,\n",
              " -0.021580799,\n",
              " -0.03839475,\n",
              " 0.015841262,\n",
              " 0.0064048977,\n",
              " 0.00884391,\n",
              " -0.012423147,\n",
              " -0.0060453643,\n",
              " 0.02140582,\n",
              " -0.008806144,\n",
              " 0.037548866,\n",
              " -0.031451035,\n",
              " -0.023714807,\n",
              " 0.009285378,\n",
              " -0.00047728888,\n",
              " -0.018435625,\n",
              " 0.010426646,\n",
              " 0.07636751,\n",
              " 0.07757101,\n",
              " 0.03991539,\n",
              " 0.06917671,\n",
              " 0.0016628958,\n",
              " -0.091617554,\n",
              " -0.026956096,\n",
              " -0.013664855,\n",
              " -0.054339245,\n",
              " -0.08310413,\n",
              " -0.025953747,\n",
              " -0.033568814,\n",
              " 0.034448486,\n",
              " 0.0032975704,\n",
              " 0.015053127,\n",
              " -0.005894113,\n",
              " 0.029622843,\n",
              " -0.08048511,\n",
              " -0.013663298,\n",
              " -0.029040001,\n",
              " -0.043319844,\n",
              " -0.058795128,\n",
              " -0.030551378,\n",
              " -0.03321159,\n",
              " 0.03361637,\n",
              " -0.0583398,\n",
              " 0.048462477,\n",
              " -0.019440303,\n",
              " -0.002224581,\n",
              " -0.013960494,\n",
              " 0.043021582,\n",
              " 0.028916152,\n",
              " 0.013766024,\n",
              " 0.029295404,\n",
              " 0.028375948,\n",
              " -0.012632167,\n",
              " 0.009296993,\n",
              " -0.029445387,\n",
              " -0.00021846336,\n",
              " -0.0015232554,\n",
              " 0.013555971,\n",
              " -0.06027861,\n",
              " 0.0022099994,\n",
              " 0.0020799884,\n",
              " -0.012007119,\n",
              " 0.008591618,\n",
              " 0.048698094,\n",
              " 0.047572628,\n",
              " 0.017961571,\n",
              " -0.04480692,\n",
              " 0.01596784,\n",
              " 0.026624791,\n",
              " 0.04592755,\n",
              " 0.023339162,\n",
              " 0.012650808,\n",
              " 0.014623401,\n",
              " 0.059190698,\n",
              " 0.053951625,\n",
              " -0.012018796,\n",
              " 0.025127882,\n",
              " 0.013777736,\n",
              " 0.008772696,\n",
              " 0.06242213,\n",
              " -0.02985679,\n",
              " 0.015534353,\n",
              " 0.008786879,\n",
              " 0.0021978319,\n",
              " 0.011586468,\n",
              " -0.02561069,\n",
              " 0.031158268,\n",
              " -0.07818875,\n",
              " -0.025129631,\n",
              " -0.15375015,\n",
              " -0.028578915,\n",
              " -0.019318363,\n",
              " -0.020512082,\n",
              " -0.006895941,\n",
              " 0.025423868,\n",
              " 0.016258566,\n",
              " -0.013225587,\n",
              " 0.07140959,\n",
              " -0.03596512,\n",
              " -0.027768183,\n",
              " 0.018441642,\n",
              " 0.0016754153,\n",
              " -0.009046769,\n",
              " 0.0068947347,\n",
              " 0.010378478,\n",
              " -0.004311906,\n",
              " -0.036049224,\n",
              " 0.0429449,\n",
              " -0.00087607105,\n",
              " -0.021248715,\n",
              " 0.039782412,\n",
              " 0.03250818,\n",
              " 0.050120413,\n",
              " -0.011048507,\n",
              " 0.0046043964,\n",
              " 0.012511691,\n",
              " 0.024216538,\n",
              " 0.01150548,\n",
              " -0.02745774,\n",
              " -0.0051559354,\n",
              " -0.007308025,\n",
              " 0.019659724,\n",
              " -0.022055222,\n",
              " 0.00027937818,\n",
              " 0.044644978,\n",
              " 0.013013166,\n",
              " 0.005594769,\n",
              " -0.009612895,\n",
              " -0.025433125,\n",
              " 0.07727911,\n",
              " 0.031199932,\n",
              " 0.038574066,\n",
              " 0.007593594,\n",
              " -0.018812446,\n",
              " -0.012732279,\n",
              " -0.0003708816,\n",
              " -0.017922256,\n",
              " -0.012979617,\n",
              " -0.048761148,\n",
              " 0.013668185,\n",
              " 0.039892722,\n",
              " 0.01600722,\n",
              " -0.015836455,\n",
              " 0.05010714,\n",
              " -0.023271231,\n",
              " -0.0015552061,\n",
              " 0.008744261,\n",
              " -0.0021387269,\n",
              " -0.0138165355,\n",
              " -0.011505079,\n",
              " 0.0383287,\n",
              " 0.032180313,\n",
              " -0.06972529,\n",
              " -0.04253552,\n",
              " -0.035327293,\n",
              " -0.03087898,\n",
              " -0.008225923,\n",
              " -0.057406187,\n",
              " 0.06683099,\n",
              " -0.021594517,\n",
              " -0.005994046,\n",
              " 0.022828693,\n",
              " 0.022839233,\n",
              " -0.03893198,\n",
              " 0.05164902,\n",
              " 0.057929542,\n",
              " 0.021702295,\n",
              " 0.006756328,\n",
              " 0.0020226631,\n",
              " -0.04540625,\n",
              " 0.0411206,\n",
              " -0.0064312797,\n",
              " 0.046170663,\n",
              " -0.02217186,\n",
              " -0.04092706,\n",
              " 0.09603613,\n",
              " 0.010617954,\n",
              " -0.019166118,\n",
              " 0.004992475,\n",
              " 0.08547908,\n",
              " 0.0042816126,\n",
              " -0.011178713,\n",
              " -0.036315963,\n",
              " -0.0397458,\n",
              " 0.0035810445,\n",
              " -0.0073561026,\n",
              " -0.0011699863,\n",
              " -0.058471896,\n",
              " -0.01679719,\n",
              " -0.022455424,\n",
              " 0.004091696,\n",
              " -0.001995662,\n",
              " 0.03065391,\n",
              " 0.00027773026,\n",
              " -0.013209596,\n",
              " 0.0062156287,\n",
              " -0.0019772635,\n",
              " 0.026441118,\n",
              " -0.08084242,\n",
              " 0.00813773,\n",
              " 0.011137467,\n",
              " 0.025003403,\n",
              " 0.039739534,\n",
              " 0.04352838,\n",
              " -0.010362335,\n",
              " -0.005326726,\n",
              " 0.026611479,\n",
              " 0.03678279,\n",
              " 0.020289298,\n",
              " -0.00024059122,\n",
              " 0.028297735,\n",
              " -0.058491826,\n",
              " -0.019358084,\n",
              " -0.009499252,\n",
              " -0.013873281,\n",
              " 0.0032195828,\n",
              " 0.04456959,\n",
              " -0.0029241447,\n",
              " 0.008418838,\n",
              " 0.03416341,\n",
              " 0.012652945,\n",
              " -0.030445265,\n",
              " 0.015912598,\n",
              " -0.015061086,\n",
              " -0.004397588,\n",
              " -0.076873265,\n",
              " -0.022693606,\n",
              " -0.017148094,\n",
              " 0.008034367,\n",
              " -0.021009823,\n",
              " -0.013641983,\n",
              " -0.030586809,\n",
              " 0.063050985,\n",
              " -0.0058573247,\n",
              " -0.03037537,\n",
              " 0.06859101,\n",
              " 0.007416954,\n",
              " 0.013147328,\n",
              " -0.03752198,\n",
              " -0.030428246,\n",
              " 0.053029884,\n",
              " -0.022908261,\n",
              " 0.05607778,\n",
              " 0.05492161,\n",
              " 0.013270513,\n",
              " 0.0070847725,\n",
              " 0.029913813,\n",
              " -0.010489726,\n",
              " 0.007844949,\n",
              " 0.07760543,\n",
              " -0.03246045,\n",
              " -0.013374177,\n",
              " -0.027256502,\n",
              " -0.004605633,\n",
              " 0.030556176,\n",
              " -0.025936672,\n",
              " 0.04827568,\n",
              " 0.044143107,\n",
              " -0.01873767,\n",
              " -0.031347197,\n",
              " -0.026854562,\n",
              " 0.031382143,\n",
              " 0.013992115,\n",
              " 0.021271078,\n",
              " 0.027894089,\n",
              " -0.024505738,\n",
              " -0.07478747,\n",
              " 0.013540895,\n",
              " -0.017613562,\n",
              " 0.01590729,\n",
              " 0.017141188,\n",
              " 0.04708115,\n",
              " 0.019568153,\n",
              " 0.091656715,\n",
              " -0.004865123,\n",
              " -0.019308813,\n",
              " -0.0053966944,\n",
              " -0.0278622,\n",
              " 0.012864926,\n",
              " -0.061174583,\n",
              " -0.041453917,\n",
              " 0.075621046,\n",
              " -0.0070187845,\n",
              " 0.01906601,\n",
              " 0.0054263994,\n",
              " 0.012566397,\n",
              " -0.019087587,\n",
              " -0.043440256,\n",
              " 0.041887432,\n",
              " -0.014445988,\n",
              " 0.04691199,\n",
              " -0.0016946428,\n",
              " 0.071609624,\n",
              " -0.024095738,\n",
              " -0.029031072,\n",
              " 0.0023203683,\n",
              " 0.014458297,\n",
              " 0.010492939,\n",
              " -0.005559697,\n",
              " 0.025936546,\n",
              " 0.009190466,\n",
              " -0.0027095198,\n",
              " -0.013050847,\n",
              " -0.020220019,\n",
              " 0.056056578,\n",
              " 0.03650916,\n",
              " 0.019963812,\n",
              " -0.013089996,\n",
              " 0.05316529,\n",
              " 0.02269551,\n",
              " 0.0141424555,\n",
              " 0.022176167,\n",
              " 0.020095332,\n",
              " 0.011605739,\n",
              " 0.0020735825,\n",
              " 0.003068887,\n",
              " 0.009191726,\n",
              " 0.025579473,\n",
              " -0.008541693,\n",
              " -0.03808776,\n",
              " 0.04622485,\n",
              " -0.029493613,\n",
              " 0.07711512,\n",
              " 0.014877751,\n",
              " -0.029058069,\n",
              " 0.025354061,\n",
              " 0.03530013,\n",
              " 0.0014759303,\n",
              " -0.021830602,\n",
              " 0.02163411,\n",
              " 0.025770994,\n",
              " -0.06815127,\n",
              " 0.021125901,\n",
              " 0.0019301678,\n",
              " 0.002210321,\n",
              " -0.00305364,\n",
              " -0.021801703,\n",
              " -0.051318586,\n",
              " -0.033611473,\n",
              " 0.012315321,\n",
              " 0.0072091133,\n",
              " 0.0014005301,\n",
              " -0.023998646,\n",
              " 0.0026291292,\n",
              " -0.015098267,\n",
              " -0.011970929,\n",
              " -0.030961366,\n",
              " 0.021716155,\n",
              " 0.019829318,\n",
              " -0.0408635,\n",
              " -0.0088035865,\n",
              " 0.02354545,\n",
              " -0.035932545,\n",
              " 0.061214827,\n",
              " 0.005042119,\n",
              " 0.053205136,\n",
              " 0.012700384,\n",
              " -0.0013505232,\n",
              " -0.02219724,\n",
              " 0.017605027,\n",
              " 0.0109856445,\n",
              " -0.0046386044,\n",
              " -0.007891236,\n",
              " -0.025283262,\n",
              " 0.05328586,\n",
              " 0.009774557,\n",
              " -0.036375165,\n",
              " -0.026973603,\n",
              " -0.024808131,\n",
              " -0.0313296,\n",
              " -0.012596162,\n",
              " 0.02893709,\n",
              " 0.007063438,\n",
              " 0.012943186,\n",
              " -0.014554011,\n",
              " 0.022427145,\n",
              " 0.009753417,\n",
              " -0.030994788,\n",
              " -0.095215425,\n",
              " -0.015310255,\n",
              " -0.03325391,\n",
              " 0.0049246587,\n",
              " -0.009970491,\n",
              " 0.0069147805,\n",
              " 0.049630444,\n",
              " -0.051078644,\n",
              " 0.0710505,\n",
              " -0.07212227,\n",
              " 0.014028713,\n",
              " -0.0039652484,\n",
              " -0.030573502,\n",
              " 0.035072844,\n",
              " -0.025866162,\n",
              " -0.033041764,\n",
              " 0.023931714,\n",
              " 0.014035653,\n",
              " -0.015747368,\n",
              " -0.042261735,\n",
              " 0.0028005617,\n",
              " -0.0047397106,\n",
              " -0.0016750308,\n",
              " -0.007684546,\n",
              " 0.022748126,\n",
              " -0.05401668,\n",
              " 0.022321891,\n",
              " 0.05747806,\n",
              " -0.032267727,\n",
              " -0.0018111368,\n",
              " 0.021240143,\n",
              " 0.015935048,\n",
              " -0.017525177,\n",
              " 0.035998467,\n",
              " 0.009683403,\n",
              " -0.0242088,\n",
              " 0.015660904,\n",
              " 0.048446238,\n",
              " 0.019246493,\n",
              " -0.048423514,\n",
              " 0.06908145,\n",
              " -0.049179938,\n",
              " -0.013505337,\n",
              " 0.061395,\n",
              " 0.028108947,\n",
              " -0.033591855,\n",
              " -0.038586985,\n",
              " 0.041065104,\n",
              " -0.022049988,\n",
              " -0.010130617,\n",
              " 0.038356528,\n",
              " -0.034783028,\n",
              " -0.043130342,\n",
              " -0.0005808886,\n",
              " -0.04879479,\n",
              " 0.0053223893,\n",
              " -0.0228413,\n",
              " 0.045616776,\n",
              " -0.007167411,\n",
              " -0.03421718,\n",
              " 0.0073885787,\n",
              " 0.040428832,\n",
              " 0.025090184,\n",
              " -0.024634155,\n",
              " -0.024775982,\n",
              " 0.0052057267,\n",
              " 0.05093614,\n",
              " 0.042788316,\n",
              " 0.024823798,\n",
              " -0.029019883,\n",
              " 0.03016593,\n",
              " 0.021206478,\n",
              " -0.012097347,\n",
              " 0.002654971,\n",
              " 0.029118014,\n",
              " 0.018068524,\n",
              " -0.017020179,\n",
              " -0.022816472,\n",
              " -0.063054584,\n",
              " -0.00427345,\n",
              " 0.026169498,\n",
              " 0.08122146,\n",
              " -0.048106845,\n",
              " -0.021307718,\n",
              " 0.000573938,\n",
              " -0.041324914,\n",
              " -0.033583045,\n",
              " 0.018485945,\n",
              " -0.009407308,\n",
              " 0.01742343,\n",
              " 0.0328296,\n",
              " -0.050955404,\n",
              " 0.051715422,\n",
              " -0.06056332,\n",
              " -0.058261856,\n",
              " -0.009281477,\n",
              " -0.011629536,\n",
              " -0.052582953,\n",
              " 0.0048258896,\n",
              " 0.04804673,\n",
              " -0.007417617,\n",
              " -0.006769257,\n",
              " -0.013769851,\n",
              " -0.07769279,\n",
              " 0.022878287,\n",
              " -0.009451909,\n",
              " 0.046827216,\n",
              " 0.043176863,\n",
              " 0.014981078,\n",
              " -0.012839531,\n",
              " 0.02928201,\n",
              " -0.0042603286,\n",
              " 0.014360433,\n",
              " 0.028610492,\n",
              " 0.018387672,\n",
              " -0.0038107908,\n",
              " -0.03366308,\n",
              " 0.0053267474,\n",
              " 0.0572671,\n",
              " 0.0006855626,\n",
              " -0.027601944,\n",
              " -0.07033053,\n",
              " 0.028943876,\n",
              " -0.033198193,\n",
              " 0.053086396,\n",
              " 0.07436674,\n",
              " 0.020729132,\n",
              " -0.031300083,\n",
              " 0.049042925,\n",
              " -0.01617157,\n",
              " -0.02372223,\n",
              " -0.017318686,\n",
              " 0.02420875,\n",
              " 0.00021474871,\n",
              " -0.008650192,\n",
              " 0.06548937,\n",
              " -0.034736387,\n",
              " -0.058506683,\n",
              " -0.049474478,\n",
              " -0.012243532,\n",
              " -0.013708403,\n",
              " -0.055309515,\n",
              " -0.013009869,\n",
              " -0.04248218,\n",
              " -0.02547653,\n",
              " -0.084748425,\n",
              " 0.004215639,\n",
              " 0.038781103,\n",
              " 0.06744112,\n",
              " 0.04144784,\n",
              " -0.002465784,\n",
              " -0.017555721,\n",
              " -0.005858821,\n",
              " 0.047553077,\n",
              " 0.047972098,\n",
              " -0.021811942,\n",
              " 0.019362554,\n",
              " -0.021370107,\n",
              " -0.0034753596,\n",
              " -0.08682825,\n",
              " 1.2328685e-05,\n",
              " 0.0107819205,\n",
              " -0.033175968]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result['embedding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzZvofrq6Uw5",
        "outputId": "93af6173-3689-40b5-f9cf-c371e8f2f7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "result : Dict = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=[\n",
        "        \"What is the meaning of life?\",\n",
        "        \"How much wood would a woodchuck chuck?\",\n",
        "        \"How does the brain work?\",\n",
        "    ],\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of list of strings\",\n",
        ")\n",
        "\n",
        "# A list of inputs > A list of vectors output\n",
        "for v in result[\"embedding\"]:\n",
        "    print(str(v)[:50], \"... TRIMMED ...\", len(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "m7l_BiIR6XZ_",
        "outputId": "a7ab2061-19fc-4785-c5e8-342e7823ada3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.036453027, 0.033254996, -0.03970925, -0.002628 ... TRIMMED ... 768\n",
            "[-0.01591948, 0.032582663, -0.081024624, -0.011298 ... TRIMMED ... 768\n",
            "[0.00037063024, 0.03763057, -0.122695684, -0.00951 ... TRIMMED ... 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Vector Stores & Retreival using Chroma DB"
      ],
      "metadata": {
        "id": "WdWneztc6bKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUmO8M5x6c0J",
        "outputId": "672f8089-29a6-4ea7-a856-e158a881ad3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "oJFSWDi-6iJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
        "        metadata={\"source\": \"fish-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
        "        metadata={\"source\": \"bird-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "JSGQcfQX6kwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain- google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BqKkgWT6nbQ",
        "outputId": "b253ee8d-1c6d-4fe8-f16c-e70f1bde244e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'langchain-': Expected end or semicolon (after name and no valid version specifier)\n",
            "    langchain-\n",
            "             ^\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwXlyz3j7Ztf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}